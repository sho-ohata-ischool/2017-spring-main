{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 0\n",
    "\n",
    "This notebook will help verify that you're all set up with the Python packages we'll be using this semester.\n",
    "\n",
    "**Your task:** just run the cells below, and verify that the output is as expected. If anything looks wrong, weird, or crashes, update your Python installation or contact the course staff. We don't want library issues to get in the way of the real coursework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version 1.11.1 is OK\n",
      "matplotlib version 1.5.3 is OK\n",
      "pandas version 0.18.1 is OK\n",
      "nltk version 3.2.1 is OK\n",
      "tensorflow version 0.12.1 is OK\n"
     ]
    }
   ],
   "source": [
    "# Version checks\n",
    "import importlib\n",
    "def version_check(libname, min_version):\n",
    "    m = importlib.import_module(libname)\n",
    "    print \"%s version %s is\" % (libname, m.__version__),\n",
    "    print (\"OK\" if m.__version__ >= min_version \n",
    "           else \"out-of-date. Please upgrade!\")\n",
    "    \n",
    "version_check(\"numpy\", \"1.11\")\n",
    "version_check(\"matplotlib\", \"1.5\")\n",
    "version_check(\"pandas\", \"0.18\")\n",
    "version_check(\"nltk\", \"3.2\")\n",
    "version_check(\"tensorflow\", \"0.12.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "We'll be using [TensorFlow](tensorflow.org) to build deep learning models this semester. TensorFlow is a whole programming system in itself, based around the idea of a computation graph and deferred execution. We'll be talking a lot more about it in Assignment 1, but for now you should just test that it loads on your system.\n",
    "\n",
    "Run the cell below; you should see:\n",
    "```\n",
    "Hello, TensorFlow!\n",
    "42\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "sess = tf.Session()\n",
    "print sess.run(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print sess.run(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) You can also test one of the built-in models. This will train a CNN classifier on the MNIST handwriting dataset. It will generate lots of output, and may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Initialized!\n",
      "Step 0 (epoch 0.00), 20.0 ms\n",
      "Minibatch loss: 8.334, learning rate: 0.010000\n",
      "Minibatch error: 85.9%\n",
      "Validation error: 84.6%\n",
      "Step 100 (epoch 0.12), 242.3 ms\n",
      "Minibatch loss: 3.250, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 7.6%\n",
      "Step 200 (epoch 0.23), 242.1 ms\n",
      "Minibatch loss: 3.376, learning rate: 0.010000\n",
      "Minibatch error: 12.5%\n",
      "Validation error: 4.2%\n",
      "Step 300 (epoch 0.35), 242.0 ms\n",
      "Minibatch loss: 3.176, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 3.0%\n",
      "Step 400 (epoch 0.47), 242.2 ms\n",
      "Minibatch loss: 3.216, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 2.6%\n",
      "Step 500 (epoch 0.58), 242.8 ms\n",
      "Minibatch loss: 3.179, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 2.5%\n",
      "Step 600 (epoch 0.70), 241.9 ms\n",
      "Minibatch loss: 3.130, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.2%\n",
      "Step 700 (epoch 0.81), 242.2 ms\n",
      "Minibatch loss: 2.983, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 2.2%\n",
      "Step 800 (epoch 0.93), 242.1 ms\n",
      "Minibatch loss: 3.040, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 2.1%\n",
      "Step 900 (epoch 1.05), 241.6 ms\n",
      "Minibatch loss: 2.907, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 1000 (epoch 1.16), 242.3 ms\n",
      "Minibatch loss: 2.859, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 2.0%\n",
      "Step 1100 (epoch 1.28), 241.9 ms\n",
      "Minibatch loss: 2.812, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.4%\n",
      "Step 1200 (epoch 1.40), 241.4 ms\n",
      "Minibatch loss: 2.911, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.6%\n",
      "Step 1300 (epoch 1.51), 241.0 ms\n",
      "Minibatch loss: 2.819, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 1400 (epoch 1.63), 241.3 ms\n",
      "Minibatch loss: 2.826, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.5%\n",
      "Step 1500 (epoch 1.75), 241.4 ms\n",
      "Minibatch loss: 2.905, learning rate: 0.009500\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.2%\n",
      "Step 1600 (epoch 1.86), 241.4 ms\n",
      "Minibatch loss: 2.731, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.4%\n",
      "Step 1700 (epoch 1.98), 241.8 ms\n",
      "Minibatch loss: 2.659, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 1800 (epoch 2.09), 242.2 ms\n",
      "Minibatch loss: 2.660, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.3%\n",
      "Step 1900 (epoch 2.21), 241.8 ms\n",
      "Minibatch loss: 2.636, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2000 (epoch 2.33), 242.1 ms\n",
      "Minibatch loss: 2.619, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2100 (epoch 2.44), 241.8 ms\n",
      "Minibatch loss: 2.568, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 2200 (epoch 2.56), 241.5 ms\n",
      "Minibatch loss: 2.572, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2300 (epoch 2.68), 241.8 ms\n",
      "Minibatch loss: 2.593, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2400 (epoch 2.79), 241.2 ms\n",
      "Minibatch loss: 2.503, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2500 (epoch 2.91), 242.1 ms\n",
      "Minibatch loss: 2.467, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2600 (epoch 3.03), 241.2 ms\n",
      "Minibatch loss: 2.469, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.4%\n",
      "Step 2700 (epoch 3.14), 241.5 ms\n",
      "Minibatch loss: 2.493, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 2800 (epoch 3.26), 241.9 ms\n",
      "Minibatch loss: 2.456, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2900 (epoch 3.37), 241.4 ms\n",
      "Minibatch loss: 2.487, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 3000 (epoch 3.49), 241.4 ms\n",
      "Minibatch loss: 2.389, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 3100 (epoch 3.61), 242.1 ms\n",
      "Minibatch loss: 2.370, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3200 (epoch 3.72), 241.9 ms\n",
      "Minibatch loss: 2.338, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3300 (epoch 3.84), 241.7 ms\n",
      "Minibatch loss: 2.330, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 3400 (epoch 3.96), 242.0 ms\n",
      "Minibatch loss: 2.288, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3500 (epoch 4.07), 241.7 ms\n",
      "Minibatch loss: 2.282, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3600 (epoch 4.19), 241.4 ms\n",
      "Minibatch loss: 2.250, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3700 (epoch 4.31), 241.9 ms\n",
      "Minibatch loss: 2.228, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3800 (epoch 4.42), 241.8 ms\n",
      "Minibatch loss: 2.215, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3900 (epoch 4.54), 241.4 ms\n",
      "Minibatch loss: 2.218, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4000 (epoch 4.65), 241.5 ms\n",
      "Minibatch loss: 2.259, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 4100 (epoch 4.77), 242.0 ms\n",
      "Minibatch loss: 2.171, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4200 (epoch 4.89), 241.4 ms\n",
      "Minibatch loss: 2.173, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4300 (epoch 5.00), 241.8 ms\n",
      "Minibatch loss: 2.203, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4400 (epoch 5.12), 241.1 ms\n",
      "Minibatch loss: 2.130, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 4500 (epoch 5.24), 241.2 ms\n",
      "Minibatch loss: 2.201, learning rate: 0.007738\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.1%\n",
      "Step 4600 (epoch 5.35), 241.5 ms\n",
      "Minibatch loss: 2.091, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4700 (epoch 5.47), 241.2 ms\n",
      "Minibatch loss: 2.103, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4800 (epoch 5.59), 241.0 ms\n",
      "Minibatch loss: 2.065, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4900 (epoch 5.70), 241.2 ms\n",
      "Minibatch loss: 2.081, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 5000 (epoch 5.82), 241.5 ms\n",
      "Minibatch loss: 2.114, learning rate: 0.007738\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 0.8%\n",
      "Step 5100 (epoch 5.93), 241.2 ms\n",
      "Minibatch loss: 2.004, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5200 (epoch 6.05), 241.9 ms\n",
      "Minibatch loss: 2.081, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 5300 (epoch 6.17), 241.8 ms\n",
      "Minibatch loss: 1.971, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5400 (epoch 6.28), 241.3 ms\n",
      "Minibatch loss: 1.959, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5500 (epoch 6.40), 241.8 ms\n",
      "Minibatch loss: 1.979, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 5600 (epoch 6.52), 241.7 ms\n",
      "Minibatch loss: 1.940, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 5700 (epoch 6.63), 241.8 ms\n",
      "Minibatch loss: 1.914, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5800 (epoch 6.75), 241.8 ms\n",
      "Minibatch loss: 1.898, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 5900 (epoch 6.87), 241.6 ms\n",
      "Minibatch loss: 1.890, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6000 (epoch 6.98), 241.2 ms\n",
      "Minibatch loss: 1.893, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6100 (epoch 7.10), 241.1 ms\n",
      "Minibatch loss: 1.865, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6200 (epoch 7.21), 241.3 ms\n",
      "Minibatch loss: 1.843, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6300 (epoch 7.33), 241.8 ms\n",
      "Minibatch loss: 1.837, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6400 (epoch 7.45), 240.8 ms\n",
      "Minibatch loss: 1.829, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6500 (epoch 7.56), 241.7 ms\n",
      "Minibatch loss: 1.807, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6600 (epoch 7.68), 241.8 ms\n",
      "Minibatch loss: 1.808, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6700 (epoch 7.80), 242.2 ms\n",
      "Minibatch loss: 1.784, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6800 (epoch 7.91), 241.7 ms\n",
      "Minibatch loss: 1.779, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6900 (epoch 8.03), 241.5 ms\n",
      "Minibatch loss: 1.759, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7000 (epoch 8.15), 241.9 ms\n",
      "Minibatch loss: 1.756, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7100 (epoch 8.26), 242.0 ms\n",
      "Minibatch loss: 1.736, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7200 (epoch 8.38), 241.2 ms\n",
      "Minibatch loss: 1.740, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7300 (epoch 8.49), 241.6 ms\n",
      "Minibatch loss: 1.727, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7400 (epoch 8.61), 241.0 ms\n",
      "Minibatch loss: 1.700, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 7500 (epoch 8.73), 241.2 ms\n",
      "Minibatch loss: 1.695, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7600 (epoch 8.84), 241.3 ms\n",
      "Minibatch loss: 1.735, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 7700 (epoch 8.96), 241.2 ms\n",
      "Minibatch loss: 1.666, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7800 (epoch 9.08), 241.3 ms\n",
      "Minibatch loss: 1.659, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7900 (epoch 9.19), 241.1 ms\n",
      "Minibatch loss: 1.648, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8000 (epoch 9.31), 241.5 ms\n",
      "Minibatch loss: 1.642, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8100 (epoch 9.43), 241.3 ms\n",
      "Minibatch loss: 1.630, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8200 (epoch 9.54), 241.1 ms\n",
      "Minibatch loss: 1.621, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8300 (epoch 9.66), 241.4 ms\n",
      "Minibatch loss: 1.612, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 8400 (epoch 9.77), 241.4 ms\n",
      "Minibatch loss: 1.596, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8500 (epoch 9.89), 241.6 ms\n",
      "Minibatch loss: 1.618, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Test error: 0.8%\n"
     ]
    }
   ],
   "source": [
    "# This is the same as calling python -m (...) on the command line\n",
    "# You should see a bunch of output, and a final test error around 0.8%\n",
    "# It might take a few minutes on a slower machine.\n",
    "%run -m tensorflow.models.image.mnist.convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/) is a large compilation of Python NLP packages. It includes implementations of a number of classic NLP models, as well as utilities for working with linguistic data structures, preprocessing text, and managing corpora.\n",
    "\n",
    "NLTK is included with Anaconda, but the corpora need to be downloaded separately. Be warned that this will take up around 3.2 GB of disk space if you download everything! If this is too much, you can download individual corpora as you need them through the same interface.\n",
    "\n",
    "Type the following into a Python shell on the command line. It'll open a pop-up UI with the downloader:\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "Alternatively, you can download individual corpora by name. The cell below will download the famous [Brown corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "assert(nltk.download(\"brown\"))  # should return True if successful, or already installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few sentences. Expect to see:\n",
    "```\n",
    "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
    "\n",
    "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "# Look at the first two sentences\n",
    "for s in brown.sents()[:2]:\n",
    "    print \" \".join(s)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also includes a sample of the [Penn treebank](https://www.cis.upenn.edu/~treebank/), which we'll be using later in the course for parsing and part-of-speech tagging. Here's a sample of sentences, and an example tree. Expect to see:\n",
    "```\n",
    "The top money funds are currently yielding well over 9 % .\n",
    "\n",
    "(S\n",
    "  (NP-SBJ (DT The) (JJ top) (NN money) (NNS funds))\n",
    "  (VP\n",
    "    (VBP are)\n",
    "    (ADVP-TMP (RB currently))\n",
    "    (VP (VBG yielding) (NP (QP (RB well) (IN over) (CD 9)) (NN %))))\n",
    "  (. .))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "\n",
      "The top money funds are currently yielding well over 9 % .\n",
      "\n",
      "(S\n",
      "  (NP-SBJ (DT The) (JJ top) (NN money) (NNS funds))\n",
      "  (VP\n",
      "    (VBP are)\n",
      "    (ADVP-TMP (RB currently))\n",
      "    (VP (VBG yielding) (NP (QP (RB well) (IN over) (CD 9)) (NN %))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "assert(nltk.download(\"treebank\"))  # should return True if successful, or already installed\n",
    "print \"\"\n",
    "from nltk.corpus import treebank\n",
    "# Look at the parse of a sentence.\n",
    "# Don't worry about what this means yet!\n",
    "idx = 45\n",
    "print \" \".join(treebank.sents()[idx])\n",
    "print \"\"\n",
    "print treebank.parsed_sents()[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the [Europarl corpus](http://www.statmt.org/europarl/), which consists of *parallel* text - a sentence and its translations to multiple languages. You should see:\n",
    "```\n",
    "ENGLISH: Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .\n",
    "```\n",
    "and its translation into French and Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/europarl_raw.zip.\n",
      "\n",
      "ENGLISH: Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .\n",
      "\n",
      "FRENCH: Reprise de la session Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances .\n",
      "\n",
      "SPANISH: Reanudación del período de sesiones Declaro reanudado el período de sesiones del Parlamento Europeo , interrumpido el viernes 17 de diciembre pasado , y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones .\n"
     ]
    }
   ],
   "source": [
    "assert(nltk.download(\"europarl_raw\"))  # should return True if successful, or already installed\n",
    "print \"\"\n",
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "idx = 0\n",
    "\n",
    "print \"ENGLISH: \" + \" \".join(europarl_raw.english.sents()[idx])\n",
    "print \"\"\n",
    "print \"FRENCH: \" + \" \".join(europarl_raw.french.sents()[idx])\n",
    "print \"\"\n",
    "print \"SPANISH: \" + \" \".join(europarl_raw.spanish.sents()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
